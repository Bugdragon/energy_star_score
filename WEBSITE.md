## 网站推荐

* 参考书籍：
    + [Hands-On Machine Learning with Scikit-Learn and Tensorflow](http://shop.oreilly.com/product/0636920052289.do)
    + [Jupyter Notebooks for this book](https://github.com/ageron/handson-ml)

* [The 7 Steps of Machine Learning](https://towardsdatascience.com/the-7-steps-of-machine-learning-2877d7e5548e)
* [An Introduction to Statistical Learning(with Applications in R)](http://www-bcf.usc.edu/~gareth/ISL/)

* Kaggle：https://www.kaggle.com/
* DataCamp：https://www.datacamp.com/
* Dataquest：https://www.dataquest.io/
* Pandas：http://pandas.pydata.org/pandas-docs/stable/
* scikit-learn：http://scikit-learn.org/stable/

* 数据源：
    + http://www.nyc.gov/html/gbee/html/plan/ll84.shtml
    + http://www.nyc.gov/html/gbee/downloads/misc/nyc_benchmarking_disclosure_data_definitions_2017.pdf

* 数据清理和格式化：
    + [How to count the NaN values in a column in Pandas DataFrame?](https://stackoverflow.com/a/39734251)
    + [What should be the allowed percentage of Missing Values?](https://discuss.analyticsvidhya.com/t/what-should-be-the-allowed-percentage-of-missing-values/2456)

* 探索性数据分析：
    + [What are outliers in the data?](https://www.itl.nist.gov/div898/handbook/prc/section1/prc16.htm)
    + [Extreme Outliers](https://people.richland.edu/james/lecture/m170/ch03-pos.html)
    + [Density Plot](https://datavizcatalogue.com/methods/density_plot.html)
    + [Exploratory Data Analysis: Kernel Density Estimation – Conceptual Foundations](https://chemicalstatistician.wordpress.com/2013/06/09/exploratory-data-analysis-kernel-density-estimation-in-r-on-ozone-pollution-data-in-new-york-and-ozonopolis/)
    + [correlation-coefficient-formula](http://www.statisticshowto.com/probability-and-statistics/correlation-coefficient-formula/)
    
* 特征工程与特征选择：
    + [Discover Feature Engineering, How to Engineer Features and How to Get Good at It](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/)
    + [An Introduction to Feature Selection](https://machinelearningmastery.com/an-introduction-to-feature-selection/)
    + [Feature Engineering: Secret to data science success](https://www.featurelabs.com/blog/secret-to-data-science-success/)
    + [sklearn.feature_selection.VarianceThreshold](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html)
    + [Issue of Multicollinearity in Python](https://statinfer.com/204-1-9-issue-of-multicollinearity-in-python/)
    + [Multicollinearity and collinearity (in multiple regression) - a tutorial](http://psychologicalstatistics.blogspot.com/2013/11/multicollinearity-and-collinearity-in.html)
    + [Dealing with multicollinearity
](https://www.kaggle.com/robertoruiz/dealing-with-multicollinearity/code)
    + [How to calculate correlation between all columns and remove highly correlated ones using python or pandas](https://stackoverflow.com/questions/29294983/how-to-calculate-correlation-between-all-columns-and-remove-highly-correlated-on#43104383)
    + [sklearn.feature_selection](http://scikit-learn.org/stable/modules/feature_selection.html)
    + [A tutorial on Principal Components Analysis](http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf)
    + [CS229 Lecture notes
Andrew Ng:Independent Components
Analysis](http://cs229.stanford.edu/notes/cs229-notes11.pdf)
    + [Feature Selection For Machine Learning in Python](https://machinelearningmastery.com/feature-selection-machine-learning-python/)
    + [Important Model Evaluation Error Metrics Everyone should know](https://www.analyticsvidhya.com/blog/2016/02/7-important-model-evaluation-error-metrics/)
    + [Single number evaluation metric](https://www.coursera.org/lecture/machine-learning-projects/single-number-evaluation-metric-wIKkC)
    + [How to compare regression models](https://people.duke.edu/~rnau/compare.htm)
    
* 在性能指标上比较几种机器学习模型：
    + [Missing-data imputation](http://www.stat.columbia.edu/~gelman/arm/missing.pdf)
    + [sklearn.preprocessing.Imputer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html)
    + [Why does sklearn Imputer need to fit?](https://stackoverflow.com/questions/46691596/why-does-sklearn-imputer-need-to-fit#46692001)
    + [What is Data Leakage](https://www.kaggle.com/dansbecker/data-leakage)
    + [Handling missing data: analysis of a challenging data set using multiple imputation](https://www.tandfonline.com/doi/full/10.1080/1743727X.2014.979146)
    + [When should I apply feature scaling for my data](https://stats.stackexchange.com/questions/121886/when-should-i-apply-feature-scaling-for-my-data)
    + [Importance of Feature Scaling](http://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html)
    + [How to Normalize and Standardize Your Machine Learning Data in Weka](https://machinelearningmastery.com/normalize-standardize-machine-learning-data-weka/)
    + [Computer Science > Machine Learning
API design for machine learning software: experiences from the scikit-learn project](https://arxiv.org/abs/1309.0238)
    + [A Gentle Introduction to Gradient Boosting](http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf)
    + [Why Kagglers Love XGBoost](http://matthewemery.ca/Why-Kagglers-Love-XGBoost/)
    + [XGBoost-code](https://www.kaggle.com/dansbecker/xgboost/code)

* 对最佳模型执行超参数调整：
    + [What is the Difference Between a Parameter and a Hyperparameter?](https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/)
    + [Tuning the hyper-parameters of an estimator](http://scikit-learn.org/stable/modules/grid_search.html)
    + [TPOT](https://epistasislab.github.io/tpot/)
    + [sklearn.ensemble.GradientBoostingRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor)
    + [Overfitting in Machine Learning: What It Is and How to Prevent It](https://elitedatascience.com/overfitting-in-machine-learning)
    + [How do you correct for overfitting for a Gradient Boosted Machine?](https://www.quora.com/How-do-you-correct-for-overfitting-for-a-Gradient-Boosted-Machine)
    + [A Kaggle Master Explains Gradient Boosting](http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/)

* 在测试集合中评估最佳模型

* 解释模型结果：
    + [The Dark Secret at the Heart of AI](https://www.technologyreview.com/s/604087/the-dark-secret-at-the-heart-of-ai/)
    + [“Why Should I Trust You?”
Explaining the Predictions of Any Classifier](https://arxiv.org/pdf/1602.04938.pdf)
    + [How are feature_importances in RandomForestClassifier determined?](https://stackoverflow.com/questions/15810339/how-are-feature-importances-in-randomforestclassifier-determined)
    + [parsimonious-model](http://www.statisticshowto.com/parsimonious-model/)
    + [lime](https://github.com/marcotcr/lime)
    + [Gradient Boosting from scratch](https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d)
    + [Gradient Boosting explained](http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html)

* 得出结论
    + [TeXstudio](https://www.texstudio.org/)
    + [LaTeX](https://www.latex-project.org/)